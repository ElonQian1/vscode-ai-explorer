// src/core/ai/MultiProviderAIClient.ts
// [module: core] [tags: AI, MultiProvider, OpenAI, Hunyuan, Fallback]
/**
 * å¤šæä¾›å•† AI å®¢æˆ·ç«¯
 * æ”¯æŒ OpenAIã€è…¾è®¯äº‘æ··å…ƒ OpenAI å…¼å®¹æ¥å£ç­‰å¤šä¸ª AI æä¾›å•†
 * 
 * æ¶æ„è¯´æ˜ï¼š
 * - ä½¿ç”¨ç»Ÿä¸€çš„ OpenAI SDK (openai npm åŒ…)
 * - é€šè¿‡åˆ‡æ¢ baseURL å’Œ apiKey å®ç°æä¾›å•†åˆ‡æ¢
 * - è…¾è®¯äº‘æ··å…ƒä½¿ç”¨å®˜æ–¹ OpenAI å…¼å®¹æ¥å£ (https://api.hunyuan.cloud.tencent.com/v1)
 * - å…·å¤‡é™çº§å’Œæ•…éšœè½¬ç§»èƒ½åŠ›
 */

import { OpenAI } from 'openai';
import * as vscode from 'vscode';
import { Logger } from '../logging/Logger';
import { AIRequest, AIResponse } from '../../shared/types';
import { RateLimiter } from './RateLimiter';

interface AIProvider {
    name: string;
    client: OpenAI;
    isAvailable: boolean;
    lastError?: string;
    lastHealthCheck: number;
}

interface ProviderConfig {
    name: string;
    apiKey: string;
    baseUrl: string;
    model: string;
}

export class MultiProviderAIClient {
    private providers: Map<string, AIProvider> = new Map();
    private rateLimiter: Map<string, number> = new Map();
    private requestQueue: Array<() => Promise<void>> = [];
    private processing = false;
    private rateLimiters: Map<string, RateLimiter> = new Map();
    
    private readonly HEALTH_CHECK_INTERVAL = 5 * 60 * 1000; // 5åˆ†é’Ÿ
    private readonly MAX_RETRIES = 3;
    private readonly RATE_LIMIT_DELAY = 1000; // 1ç§’

    constructor(private logger: Logger) {}

    /**
     * åˆå§‹åŒ– AI å®¢æˆ·ç«¯
     */
    async initialize(): Promise<void> {
        await this.loadProviderConfigs();
        this.logger.info('å¤šæä¾›å•† AI å®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ');
    }

    /**
     * å‘é€ AI è¯·æ±‚ï¼ˆå¸¦é™çº§æœºåˆ¶å’Œæ™ºèƒ½æä¾›å•†é€‰æ‹©ï¼‰
     */
    async sendRequest(request: AIRequest): Promise<AIResponse> {
        const config = vscode.workspace.getConfiguration('aiExplorer');
        let primaryProvider = config.get<string>('provider.primary', 'openai');
        const fallbackProvider = config.get<string>('provider.fallback', 'none');

        // ğŸ”§ æ™ºèƒ½æä¾›å•†é€‰æ‹©ï¼šå¦‚æœä¸»æä¾›å•†æœªé…ç½®ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°å·²é…ç½®çš„æä¾›å•†
        const openaiKey = config.get<string>('openaiApiKey');
        const hunyuanKey = config.get<string>('hunyuanApiKey');
        
        if (!this.providers.has(primaryProvider) || !this.providers.get(primaryProvider)?.isAvailable) {
            this.logger.warn(`ä¸»æä¾›å•† ${primaryProvider} æœªé…ç½®æˆ–ä¸å¯ç”¨`);
            
            // è‡ªåŠ¨é€‰æ‹©å·²é…ç½®çš„æä¾›å•†
            if (primaryProvider === 'openai' && !openaiKey && hunyuanKey) {
                this.logger.info('ğŸ”„ OpenAI æœªé…ç½®ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°è…¾è®¯æ··å…ƒ');
                primaryProvider = 'hunyuan';
                
                // æ›´æ–°é…ç½®ï¼ˆä»…æœ¬æ¬¡ä¼šè¯ï¼‰
                await config.update('provider.primary', 'hunyuan', vscode.ConfigurationTarget.Global);
                
                vscode.window.showInformationMessage(
                    'âœ… å·²è‡ªåŠ¨åˆ‡æ¢åˆ°è…¾è®¯æ··å…ƒï¼ˆæ£€æµ‹åˆ°æœªé…ç½® OpenAIï¼‰',
                    'æŸ¥çœ‹é…ç½®'
                ).then(action => {
                    if (action === 'æŸ¥çœ‹é…ç½®') {
                        vscode.commands.executeCommand('workbench.action.openSettings', 'aiExplorer.provider.primary');
                    }
                });
            } else if (primaryProvider === 'hunyuan' && !hunyuanKey && openaiKey) {
                this.logger.info('ğŸ”„ è…¾è®¯æ··å…ƒæœªé…ç½®ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ° OpenAI');
                primaryProvider = 'openai';
                
                await config.update('provider.primary', 'openai', vscode.ConfigurationTarget.Global);
                
                vscode.window.showInformationMessage(
                    'âœ… å·²è‡ªåŠ¨åˆ‡æ¢åˆ° OpenAIï¼ˆæ£€æµ‹åˆ°æœªé…ç½®è…¾è®¯æ··å…ƒï¼‰',
                    'æŸ¥çœ‹é…ç½®'
                ).then(action => {
                    if (action === 'æŸ¥çœ‹é…ç½®') {
                        vscode.commands.executeCommand('workbench.action.openSettings', 'aiExplorer.provider.primary');
                    }
                });
            }
        }

        try {
            // å°è¯•ä¸»æä¾›å•†
            this.logger.debug(`ä½¿ç”¨ä¸»æä¾›å•†: ${primaryProvider}`);
            return await this.sendToProvider(primaryProvider, request);
        } catch (primaryError) {
            const errorMsg = primaryError instanceof Error ? primaryError.message : String(primaryError);
            this.logger.error(`ä¸»æä¾›å•† ${primaryProvider} è¯·æ±‚å¤±è´¥: ${errorMsg}`, primaryError);

            // å°è¯•å¤‡ç”¨æä¾›å•†
            if (fallbackProvider && fallbackProvider !== 'none' && fallbackProvider !== primaryProvider) {
                try {
                    this.logger.info(`å°è¯•å¤‡ç”¨æä¾›å•†: ${fallbackProvider}`);
                    return await this.sendToProvider(fallbackProvider, request);
                } catch (fallbackError) {
                    const fallbackErrorMsg = fallbackError instanceof Error ? fallbackError.message : String(fallbackError);
                    this.logger.error(`å¤‡ç”¨æä¾›å•† ${fallbackProvider} ä¹Ÿå¤±è´¥: ${fallbackErrorMsg}`, fallbackError);
                }
            }

            // æ‰€æœ‰æä¾›å•†éƒ½å¤±è´¥ - æä¾›è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
            const detailedError = new Error(
                `âŒ AI ç¿»è¯‘å¤±è´¥\n\n` +
                `ä¸»æä¾›å•†: ${primaryProvider} ${!this.providers.has(primaryProvider) ? '(æœªé…ç½®)' : '(è¯·æ±‚å¤±è´¥)'}\n` +
                `é”™è¯¯ä¿¡æ¯: ${errorMsg}\n\n` +
                `ğŸ’¡ è§£å†³æ–¹æ¡ˆ:\n` +
                `1. æ£€æŸ¥ API Key æ˜¯å¦æ­£ç¡®é…ç½®\n` +
                `2. æ£€æŸ¥ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸\n` +
                `3. å°è¯•åˆ‡æ¢æä¾›å•†ï¼ˆè®¾ç½® > AI Explorer > Provider: Primaryï¼‰`
            );
            
            throw detailedError;
        }
    }

    /**
     * æ‰¹é‡ç¿»è¯‘è¯·æ±‚
     */
    async translateBatch(texts: string[]): Promise<Map<string, string>> {
        const results = new Map<string, string>();
        const batchSize = 10; // æ¯æ‰¹å¤„ç†10ä¸ª
        
        this.logger.debug(`å¼€å§‹æ‰¹é‡ç¿»è¯‘ ${texts.length} ä¸ªæ–‡æœ¬ï¼Œæ‰¹æ¬¡å¤§å°: ${batchSize}`);
        
        for (let i = 0; i < texts.length; i += batchSize) {
            const batch = texts.slice(i, i + batchSize);
            const batchPrompt = this.createBatchTranslationPrompt(batch);
            
            try {
                this.logger.debug(`ç¿»è¯‘æ‰¹æ¬¡ ${Math.floor(i / batchSize) + 1}/${Math.ceil(texts.length / batchSize)}: ${batch.join(', ')}`);
                
                const response = await this.sendRequest({
                    prompt: batchPrompt,
                    maxTokens: 1000,
                    temperature: 0.3
                });

                this.logger.debug(`æ‰¹æ¬¡ç¿»è¯‘å“åº”: ${response.content.substring(0, 200)}...`);

                const translations = this.parseBatchTranslationResponse(response.content, batch);
                
                this.logger.debug(`è§£æå¾—åˆ° ${translations.size} ä¸ªç¿»è¯‘ç»“æœ`);
                
                for (const [original, translated] of translations) {
                    results.set(original, translated);
                    this.logger.debug(`  ${original} -> ${translated}`);
                }

                // æ‰¹é‡ä¹‹é—´æ·»åŠ å»¶è¿Ÿ
                if (i + batchSize < texts.length) {
                    await this.delay(this.RATE_LIMIT_DELAY);
                }
            } catch (error) {
                this.logger.error(`æ‰¹é‡ç¿»è¯‘å¤±è´¥ (batch ${Math.floor(i / batchSize) + 1})`, error);
                
                // è®°å½•è¯¦ç»†é”™è¯¯ä¿¡æ¯
                const errorMessage = error instanceof Error ? error.message : String(error);
                this.logger.error(`é”™è¯¯è¯¦æƒ…: ${errorMessage}`, {
                    batch,
                    batchIndex: Math.floor(i / batchSize) + 1,
                    errorStack: error instanceof Error ? error.stack : undefined
                });
                
                // å•ä¸ªç¿»è¯‘ä½œä¸ºå¤‡é€‰
                for (const text of batch) {
                    try {
                        this.logger.debug(`å°è¯•å•ä¸ªç¿»è¯‘: ${text}`);
                        const response = await this.sendRequest({
                            prompt: `å°†ä»¥ä¸‹è‹±æ–‡ç¿»è¯‘æˆä¸­æ–‡ï¼Œåªè¿”å›ç¿»è¯‘ç»“æœï¼š${text}`,
                            maxTokens: 100,
                            temperature: 0.3
                        });
                        results.set(text, response.content.trim());
                        this.logger.info(`å•ä¸ªç¿»è¯‘æˆåŠŸ: ${text} -> ${response.content.trim()}`);
                        await this.delay(500); // å•ä¸ªè¯·æ±‚é—´éš”
                    } catch (singleError) {
                        this.logger.warn(`å•ä¸ªç¿»è¯‘å¤±è´¥: ${text}`, singleError);
                        const singleErrorMsg = singleError instanceof Error ? singleError.message : String(singleError);
                        this.logger.error(`å•ä¸ªç¿»è¯‘é”™è¯¯è¯¦æƒ…: ${singleErrorMsg}`);
                    }
                }
            }
        }

        this.logger.info(`æ‰¹é‡ç¿»è¯‘å®Œæˆï¼ŒæˆåŠŸ ${results.size}/${texts.length} ä¸ª`);
        return results;
    }

    /**
     * è·å–æä¾›å•†çŠ¶æ€
     */
    getProviderStatus(): Record<string, { available: boolean; lastError?: string }> {
        const status: Record<string, { available: boolean; lastError?: string }> = {};
        
        for (const [name, provider] of this.providers) {
            status[name] = {
                available: provider.isAvailable,
                lastError: provider.lastError
            };
        }

        return status;
    }

    /**
     * å¥åº·æ£€æŸ¥
     */
    async healthCheck(): Promise<void> {
        const now = Date.now();
        
        for (const [name, provider] of this.providers) {
            if (now - provider.lastHealthCheck > this.HEALTH_CHECK_INTERVAL) {
                try {
                    await this.testProvider(provider);
                    provider.isAvailable = true;
                    provider.lastError = undefined;
                } catch (error) {
                    provider.isAvailable = false;
                    provider.lastError = error instanceof Error ? error.message : String(error);
                }
                provider.lastHealthCheck = now;
            }
        }
    }

    private async sendToProvider(providerName: string, request: AIRequest): Promise<AIResponse> {
        const provider = this.providers.get(providerName);
        if (!provider) {
            throw new Error(`æœªçŸ¥çš„æä¾›å•†: ${providerName}`);
        }

        if (!provider.isAvailable) {
            throw new Error(`æä¾›å•† ${providerName} ä¸å¯ç”¨: ${provider.lastError}`);
        }

        // é€Ÿç‡é™åˆ¶
        await this.enforceRateLimit(providerName);

        let lastError: Error | null = null;
        
        for (let attempt = 1; attempt <= this.MAX_RETRIES; attempt++) {
            try {
                const model = this.getModelForProvider(providerName);
                this.logger.debug(`[${providerName}] å‘é€è¯·æ±‚: model=${model}, maxTokens=${request.maxTokens}`);
                
                const response = await provider.client.chat.completions.create({
                    model: model,
                    messages: [{ role: 'user', content: request.prompt }],
                    max_tokens: request.maxTokens,
                    temperature: request.temperature || 0.7
                });

                const content = response.choices[0]?.message?.content;
                if (!content) {
                    throw new Error('AI å“åº”ä¸ºç©º');
                }

                this.logger.debug(`[${providerName}] è¯·æ±‚æˆåŠŸ: tokens=${response.usage?.total_tokens || 0}`);
                
                return {
                    content: content.trim(),
                    model: response.model,
                    usage: {
                        promptTokens: response.usage?.prompt_tokens || 0,
                        completionTokens: response.usage?.completion_tokens || 0,
                        totalTokens: response.usage?.total_tokens || 0
                    }
                };
            } catch (error) {
                lastError = error instanceof Error ? error : new Error(String(error));
                const errorMsg = lastError.message || String(error);
                
                // æ£€æŸ¥æ˜¯å¦æ˜¯429é€Ÿç‡é™åˆ¶é”™è¯¯
                const is429Error = errorMsg.includes('429') || errorMsg.includes('Too Many Requests');
                
                this.logger.warn(
                    `[${providerName}] è¯·æ±‚å¤±è´¥ (å°è¯• ${attempt}/${this.MAX_RETRIES}): ${errorMsg}`,
                    error
                );
                
                if (attempt < this.MAX_RETRIES) {
                    let delayTime = Math.pow(2, attempt) * 1000; // æŒ‡æ•°é€€é¿
                    
                    // å¯¹äº429é”™è¯¯ï¼Œä½¿ç”¨æ›´é•¿çš„ç­‰å¾…æ—¶é—´
                    if (is429Error) {
                        delayTime = Math.min(60000, delayTime * 5); // æœ€é•¿1åˆ†é’Ÿ
                        this.logger.info(`é‡åˆ°é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾… ${delayTime/1000} ç§’åé‡è¯•`);
                        
                        // é‡ç½®è¯¥æä¾›å•†çš„é€Ÿç‡é™åˆ¶å™¨
                        const limiter = this.rateLimiters.get(providerName);
                        if (limiter) {
                            limiter.reset();
                        }
                    }
                    
                    await this.delay(delayTime);
                }
            }
        }

        provider.isAvailable = false;
        provider.lastError = lastError?.message;
        throw lastError || new Error('æœªçŸ¥é”™è¯¯');
    }

    private async loadProviderConfigs(): Promise<void> {
        const config = vscode.workspace.getConfiguration('aiExplorer');
        
        // OpenAI é…ç½®
        const openaiKey = config.get<string>('openaiApiKey');
        const openaiBaseUrl = config.get<string>('openaiBaseUrl', 'https://api.openai.com/v1');
        
        if (openaiKey) {
            try {
                const client = new OpenAI({
                    apiKey: openaiKey,
                    baseURL: openaiBaseUrl
                });

                this.providers.set('openai', {
                    name: 'OpenAI',
                    client,
                    isAvailable: true,
                    lastHealthCheck: 0
                });
                this.logger.debug('OpenAI æä¾›å•†é…ç½®å®Œæˆ');
            } catch (error) {
                this.logger.error('OpenAI æä¾›å•†é…ç½®å¤±è´¥', error);
            }
        }

        // è…¾è®¯äº‘æ··å…ƒé…ç½®ï¼ˆOpenAI å…¼å®¹æ¥å£ï¼‰
        // å‚è€ƒæ–‡æ¡£ï¼šhttps://cloud.tencent.com/document/product/1729/111007
        const hunyuanKey = config.get<string>('hunyuanApiKey');
        const hunyuanBaseUrl = config.get<string>('hunyuanBaseUrl', 'https://api.hunyuan.cloud.tencent.com/v1');
        
        if (hunyuanKey) {
            try {
                // ä½¿ç”¨æ ‡å‡† OpenAI SDKï¼Œä»…åˆ‡æ¢ baseURL å’Œ apiKey
                const client = new OpenAI({
                    apiKey: hunyuanKey,  // è…¾è®¯äº‘æ··å…ƒä¸“ç”¨ API Keyï¼ˆé SecretId/SecretKeyï¼‰
                    baseURL: hunyuanBaseUrl,  // OpenAI å…¼å®¹æ¥å£ç«¯ç‚¹
                    defaultHeaders: {
                        'Content-Type': 'application/json'
                    }
                });

                this.providers.set('hunyuan', {
                    name: 'è…¾è®¯æ··å…ƒ',
                    client,
                    isAvailable: true,
                    lastHealthCheck: 0
                });
                this.logger.debug('è…¾è®¯äº‘æ··å…ƒ OpenAI å…¼å®¹æ¥å£é…ç½®å®Œæˆ');
            } catch (error) {
                this.logger.error('è…¾è®¯äº‘æ··å…ƒé…ç½®å¤±è´¥', error);
            }
        }

        if (this.providers.size === 0) {
            this.logger.warn('âŒ æ²¡æœ‰é…ç½®ä»»ä½• AI æä¾›å•†');
            
            // æ˜¾ç¤ºå‹å¥½çš„æç¤º
            vscode.window.showWarningMessage(
                'âš ï¸ æœªé…ç½®ä»»ä½• AI æä¾›å•†ï¼Œç¿»è¯‘åŠŸèƒ½å°†ä¸å¯ç”¨\n\nè¯·é…ç½® OpenAI æˆ–è…¾è®¯æ··å…ƒ API Key',
                'é…ç½® OpenAI',
                'é…ç½®è…¾è®¯æ··å…ƒ',
                'æŸ¥çœ‹æ–‡æ¡£'
            ).then(action => {
                if (action === 'é…ç½® OpenAI') {
                    vscode.commands.executeCommand('aiExplorer.setOpenAIKey');
                } else if (action === 'é…ç½®è…¾è®¯æ··å…ƒ') {
                    vscode.commands.executeCommand('aiExplorer.setHunyuanKey');
                } else if (action === 'æŸ¥çœ‹æ–‡æ¡£') {
                    vscode.env.openExternal(vscode.Uri.parse('https://github.com/ElonQian1/vscode-ai-explorer#é…ç½®-ai-æœåŠ¡'));
                }
            });
        } else {
            // æ˜¾ç¤ºå·²é…ç½®çš„æä¾›å•†
            const configuredProviders = Array.from(this.providers.keys()).join(', ');
            this.logger.info(`âœ… å·²é…ç½®æä¾›å•†: ${configuredProviders}`);
            
            // æ£€æŸ¥ä¸»æä¾›å•†æ˜¯å¦å·²é…ç½®
            const primaryProvider = config.get<string>('provider.primary', 'openai');
            if (!this.providers.has(primaryProvider)) {
                this.logger.warn(`âš ï¸ ä¸»æä¾›å•† ${primaryProvider} æœªé…ç½®ï¼Œå¯èƒ½å¯¼è‡´ç¿»è¯‘å¤±è´¥`);
                
                // è‡ªåŠ¨åˆ‡æ¢åˆ°å·²é…ç½®çš„æä¾›å•†
                const availableProvider = Array.from(this.providers.keys())[0];
                this.logger.info(`ğŸ”„ è‡ªåŠ¨åˆ‡æ¢ä¸»æä¾›å•†ä¸º: ${availableProvider}`);
                await config.update('provider.primary', availableProvider, vscode.ConfigurationTarget.Global);
                
                vscode.window.showInformationMessage(
                    `âœ… å·²è‡ªåŠ¨è®¾ç½®ä¸»æä¾›å•†ä¸º ${availableProvider}`,
                    'æŸ¥çœ‹é…ç½®'
                ).then(action => {
                    if (action === 'æŸ¥çœ‹é…ç½®') {
                        vscode.commands.executeCommand('workbench.action.openSettings', 'aiExplorer.provider');
                    }
                });
            }
        }
    }

    private getModelForProvider(providerName: string): string {
        const config = vscode.workspace.getConfiguration('aiExplorer');
        
        switch (providerName) {
            case 'openai':
                return config.get<string>('model', 'gpt-3.5-turbo');
            case 'hunyuan':
                // è…¾è®¯äº‘æ··å…ƒå¯ç”¨æ¨¡å‹ï¼ˆå‚è€ƒå®˜æ–¹æ–‡æ¡£ï¼‰
                // - hunyuan-turbo: é€šç”¨ç‰ˆï¼Œæ¨èä½¿ç”¨
                // - hunyuan-turbos-longtext: é•¿æ–‡æœ¬ç‰ˆï¼Œæ”¯æŒæ›´å¤§ä¸Šä¸‹æ–‡
                // - hunyuan-lite: è½»é‡ç‰ˆï¼Œé€Ÿåº¦å¿«
                // - hunyuan-pro: ä¸“ä¸šç‰ˆï¼Œé«˜è´¨é‡
                // - hunyuan-standard: æ ‡å‡†ç‰ˆ
                return config.get<string>('hunyuanModel', 'hunyuan-turbo');
            default:
                return 'gpt-3.5-turbo';
        }
    }

    private async testProvider(provider: AIProvider): Promise<void> {
        const testResponse = await provider.client.chat.completions.create({
            model: this.getModelForProvider(provider.name.toLowerCase()),
            messages: [{ role: 'user', content: 'test' }],
            max_tokens: 5
        });

        if (!testResponse.choices[0]?.message?.content) {
            throw new Error('æµ‹è¯•è¯·æ±‚å¤±è´¥');
        }
    }

    private async enforceRateLimit(providerName: string): Promise<void> {
        // è·å–æˆ–åˆ›å»ºè¯¥æä¾›å•†çš„é€Ÿç‡é™åˆ¶å™¨
        let limiter = this.rateLimiters.get(providerName);
        if (!limiter) {
            // é’ˆå¯¹ä¸åŒæä¾›å•†ä½¿ç”¨ä¸åŒçš„é™åˆ¶ç­–ç•¥ - æ›´ä¿å®ˆçš„è®¾ç½®
            const isHunyuan = providerName.includes('hunyuan') || providerName.includes('è…¾è®¯');
            limiter = new RateLimiter(
                isHunyuan ? 3 : 5,     // å¤§å¹…é™ä½é¢‘ç‡ï¼šè…¾è®¯å…ƒå®3æ¬¡/åˆ†é’Ÿï¼Œå…¶ä»–5æ¬¡/åˆ†é’Ÿ
                60000,                 // 1åˆ†é’Ÿçª—å£
                isHunyuan ? 5000 : 3000 // å¤§å¹…å¢åŠ é—´éš”ï¼šè…¾è®¯å…ƒå®5ç§’ï¼Œå…¶ä»–3ç§’
            );
            this.rateLimiters.set(providerName, limiter);
            this.logger.info(`ä¸ºæä¾›å•† ${providerName} åˆ›å»ºé€Ÿç‡é™åˆ¶å™¨: ${isHunyuan ? 'ä¸¥æ ¼æ¨¡å¼' : 'æ ‡å‡†æ¨¡å¼'}`);
        }
        
        await limiter.waitIfNeeded();
    }

    private delay(ms: number): Promise<void> {
        return new Promise(resolve => setTimeout(resolve, ms));
    }

    private createBatchTranslationPrompt(texts: string[]): string {
        return `è¯·å°†ä»¥ä¸‹è‹±æ–‡è¯æ±‡ç¿»è¯‘æˆä¸­æ–‡ï¼Œæ¯è¡Œä¸€ä¸ªç¿»è¯‘ç»“æœï¼Œä¿æŒåŸæœ‰é¡ºåºï¼š

${texts.map((text, index) => `${index + 1}. ${text}`).join('\n')}

è¦æ±‚ï¼š
- åªè¿”å›ç¿»è¯‘ç»“æœï¼Œä¸è¦ç¼–å·
- æ¯è¡Œä¸€ä¸ªç»“æœ
- å¦‚æœæ˜¯ç¼–ç¨‹æœ¯è¯­ï¼Œä¼˜å…ˆä½¿ç”¨å¸¸è§çš„ä¸­æ–‡è¯‘å
- å¦‚æœæ˜¯æ–‡ä»¶åæˆ–ç›®å½•åï¼Œç¿»è¯‘è¦ç®€æ´å‡†ç¡®`;
    }

    private parseBatchTranslationResponse(response: string, originalTexts: string[]): Map<string, string> {
        const results = new Map<string, string>();
        const lines = response.split('\n').map(line => line.trim()).filter(line => line);
        
        for (let i = 0; i < Math.min(lines.length, originalTexts.length); i++) {
            const original = originalTexts[i];
            const translated = lines[i].replace(/^\d+\.\s*/, ''); // ç§»é™¤å¯èƒ½çš„ç¼–å·
            
            if (translated && translated !== original) {
                results.set(original, translated);
            }
        }

        return results;
    }
}